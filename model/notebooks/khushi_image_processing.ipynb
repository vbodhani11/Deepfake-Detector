{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMmgZe+JgML46aZAPBo7Rco"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"fc439cd1d2a246338ac6ec412ade7e85":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6031c85ca353482aba1840f6d850f575","IPY_MODEL_c6a2d197783546718672ec3bafd52766","IPY_MODEL_26c7d9caacca42ecafe411ce66a9493f"],"layout":"IPY_MODEL_4fad54206f5943dcbb6e25c459721934"}},"6031c85ca353482aba1840f6d850f575":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8fad6a5000d34b90b5e256a7ce900f43","placeholder":"​","style":"IPY_MODEL_3f19a69314454eb1bbe48cda3a8a83c7","value":"Videos: 100%"}},"c6a2d197783546718672ec3bafd52766":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_82457bece57c4e73a9dc156604c3533a","max":1518,"min":0,"orientation":"horizontal","style":"IPY_MODEL_36ce2fe67f9d45b2847771dc694103f1","value":1518}},"26c7d9caacca42ecafe411ce66a9493f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_21ab10a4e7e949bc8a3bf82047015c7a","placeholder":"​","style":"IPY_MODEL_ee144288f358404d8f77f19ef04b6c89","value":" 1518/1518 [5:41:54&lt;00:00, 12.74s/it]"}},"4fad54206f5943dcbb6e25c459721934":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8fad6a5000d34b90b5e256a7ce900f43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f19a69314454eb1bbe48cda3a8a83c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"82457bece57c4e73a9dc156604c3533a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36ce2fe67f9d45b2847771dc694103f1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"21ab10a4e7e949bc8a3bf82047015c7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee144288f358404d8f77f19ef04b6c89":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iiUXVnoC6WB1","executionInfo":{"status":"ok","timestamp":1759432535521,"user_tz":240,"elapsed":10137,"user":{"displayName":"Patekj","userId":"15093661387757056260"}},"outputId":"31fdfc78-9df9-448d-af9e-fa248e93f11d"},"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA available: False\n"]}],"source":["!pip -q install facenet-pytorch==2.5.3 tqdm>=4.67\n","\n","import os, sys, math, json, traceback, shutil, glob\n","from pathlib import Path\n","from typing import List, Tuple, Optional\n","\n","import numpy as np\n","import cv2\n","from PIL import Image\n","import torch\n","from facenet_pytorch import MTCNN\n","from tqdm.auto import tqdm\n","\n","print(\"CUDA available:\", torch.cuda.is_available())"]},{"cell_type":"code","source":["# =========================\n","# Config\n","# =========================\n","# If your data is in Drive, uncomment these two lines to mount:\n","# from google.colab import drive\n","# drive.mount('/content/drive')\n","\n","# Point this to your DFDC part 2 directory (where the .mp4 files are).\n","INPUT_DIR  = \"/content/drive/MyDrive/dfdc_train_part_2\"   # e.g. \"/content/drive/MyDrive/DFDC/dfdc_train_part_2\"\n","OUTPUT_DIR = \"dfdc_faces_part_2\"   # results will be written here\n","\n","# Frame sampling\n","FRAMES_PER_SECOND = 1        # sample N frames per second\n","MAX_FRAMES_PER_VIDEO = 64    # safety cap (set None to disable)\n","\n","# Face crop\n","FACE_MARGIN_RATIO = 0.2      # 20% margin around detected box\n","MIN_FACE_SIZE = 64           # skip detections smaller than this (in pixels on short edge)\n","\n","# Output image size and normalization\n","TARGET_SIZE = (299, 299)     # width, height\n","NORMALIZE_TO = \"0_1\"         # \"0_1\" or \"neg1_1\"\n","\n","# Save options\n","SAVE_JPEGS = True            # save each processed frame as .jpg\n","SAVE_NPY   = True            # save per-video tensor as .npy\n","\n","# Device for MTCNN\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Make dirs\n","Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n"],"metadata":{"id":"KWWHfYTv69FA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# Helpers\n","# =========================\n","def normalize_img(img_np: np.ndarray, mode: str = \"0_1\") -> np.ndarray:\n","    \"\"\"\n","    img_np: HxWxC in uint8\n","    returns float32 normalized image\n","    \"\"\"\n","    x = img_np.astype(np.float32) / 255.0\n","    if mode == \"neg1_1\":\n","        x = x * 2.0 - 1.0\n","    return x\n","\n","def expand_box(xyxy, margin_ratio, img_w, img_h):\n","    x1, y1, x2, y2 = xyxy\n","    w = x2 - x1\n","    h = y2 - y1\n","    cx = x1 + w / 2.0\n","    cy = y1 + h / 2.0\n","    m = margin_ratio * max(w, h)\n","    new_w = w + 2*m\n","    new_h = h + 2*m\n","    nx1 = max(0, int(round(cx - new_w/2)))\n","    ny1 = max(0, int(round(cy - new_h/2)))\n","    nx2 = min(img_w, int(round(cx + new_w/2)))\n","    ny2 = min(img_h, int(round(cy + new_h/2)))\n","    return nx1, ny1, nx2, ny2\n","\n","def pick_largest_box(boxes: np.ndarray) -> Optional[np.ndarray]:\n","    if boxes is None or len(boxes) == 0:\n","        return None\n","    areas = (boxes[:,2]-boxes[:,0]) * (boxes[:,3]-boxes[:,1])\n","    idx = int(np.argmax(areas))\n","    return boxes[idx]\n","\n","def ensure_dir(p: Path):\n","    p.mkdir(parents=True, exist_ok=True)\n"],"metadata":{"id":"rA9kdh7V7Km5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# Initialize detector\n","# =========================\n","# keep_all=True gives all faces; we'll pick the largest for consistency\n","mtcnn = MTCNN(keep_all=True, device=DEVICE, thresholds=[0.6, 0.7, 0.7])\n"],"metadata":{"id":"IXJSt2y67OLB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# Processing loop\n","# =========================\n","video_paths = sorted([p for p in Path(INPUT_DIR).glob(\"*.mp4\")])\n","\n","if not video_paths:\n","    print(f\"No .mp4 files found in {INPUT_DIR}. Check your path.\")\n","else:\n","    print(f\"Found {len(video_paths)} videos.\")\n","\n","error_log = []\n","\n","for vpath in tqdm(video_paths, desc=\"Videos\"):\n","    try:\n","        cap = cv2.VideoCapture(str(vpath))\n","        if not cap.isOpened():\n","            raise RuntimeError(\"Could not open video\")\n","\n","        fps = cap.get(cv2.CAP_PROP_FPS)\n","        fps = fps if fps and fps > 0 else 30.0\n","        step = max(1, int(round(fps / FRAMES_PER_SECOND)))\n","\n","        # Output folders\n","        vname = vpath.stem\n","        out_dir = Path(OUTPUT_DIR) / vname\n","        if SAVE_JPEGS:\n","            ensure_dir(out_dir)\n","\n","        processed_frames = []\n","        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","        save_count = 0\n","        read_idx = 0\n","\n","        while True:\n","            ret = cap.grab()  # fast skip\n","            if not ret:\n","                break\n","            if read_idx % step != 0:\n","                read_idx += 1\n","                continue\n","\n","            # retrieve frame at this index\n","            ret, frame = cap.retrieve()\n","            if not ret:\n","                read_idx += 1\n","                continue\n","\n","            # BGR -> RGB\n","            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            pil_img = Image.fromarray(rgb)\n","\n","            # detect faces\n","            boxes, probs = mtcnn.detect(pil_img)\n","            if boxes is None or len(boxes) == 0:\n","                read_idx += 1\n","                continue\n","\n","            # pick largest face\n","            box = pick_largest_box(boxes)\n","            img_h, img_w = rgb.shape[:2]\n","\n","            # expand with margin\n","            x1, y1, x2, y2 = expand_box(box, FACE_MARGIN_RATIO, img_w, img_h)\n","\n","            # skip tiny faces\n","            if min(x2-x1, y2-y1) < MIN_FACE_SIZE:\n","                read_idx += 1\n","                continue\n","\n","            # crop + resize\n","            crop = rgb[y1:y2, x1:x2]\n","            if crop.size == 0:\n","                read_idx += 1\n","                continue\n","\n","            crop_resized = cv2.resize(crop, TARGET_SIZE, interpolation=cv2.INTER_AREA)\n","\n","            # save jpeg (uint8)\n","            if SAVE_JPEGS:\n","                out_file = out_dir / f\"{vname}_{save_count:05d}.jpg\"\n","                cv2.imwrite(str(out_file), cv2.cvtColor(crop_resized, cv2.COLOR_RGB2BGR))\n","\n","            # normalized float\n","            norm = normalize_img(crop_resized, NORMALIZE_TO)\n","            processed_frames.append(norm)\n","\n","            save_count += 1\n","            read_idx += 1\n","\n","            if MAX_FRAMES_PER_VIDEO is not None and save_count >= MAX_FRAMES_PER_VIDEO:\n","                break\n","\n","        cap.release()\n","\n","        # save npy (N, 299, 299, 3), float32\n","        if SAVE_NPY and processed_frames:\n","            arr = np.stack(processed_frames, axis=0).astype(np.float32)\n","            np.save(str(Path(OUTPUT_DIR) / f\"{vname}.npy\"), arr)\n","\n","    except Exception as e:\n","        error_info = {\n","            \"video\": str(vpath),\n","            \"error\": repr(e),\n","            \"trace\": traceback.format_exc(limit=1)\n","        }\n","        error_log.append(error_info)\n","\n","# Write error log if any\n","if error_log:\n","    with open(Path(OUTPUT_DIR) / \"errors.json\", \"w\") as f:\n","        json.dump(error_log, f, indent=2)\n","    print(f\"Completed with {len(error_log)} errors. See errors.json in {OUTPUT_DIR}.\")\n","else:\n","    print(\"All videos processed successfully.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":85,"referenced_widgets":["fc439cd1d2a246338ac6ec412ade7e85","6031c85ca353482aba1840f6d850f575","c6a2d197783546718672ec3bafd52766","26c7d9caacca42ecafe411ce66a9493f","4fad54206f5943dcbb6e25c459721934","8fad6a5000d34b90b5e256a7ce900f43","3f19a69314454eb1bbe48cda3a8a83c7","82457bece57c4e73a9dc156604c3533a","36ce2fe67f9d45b2847771dc694103f1","21ab10a4e7e949bc8a3bf82047015c7a","ee144288f358404d8f77f19ef04b6c89"]},"id":"3zVDKuTG7Rcq","executionInfo":{"status":"ok","timestamp":1759453058425,"user_tz":240,"elapsed":20514517,"user":{"displayName":"Patekj","userId":"15093661387757056260"}},"outputId":"3adb2ef3-6e02-4074-9f2b-b8bd15856983"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 1518 videos.\n"]},{"output_type":"display_data","data":{"text/plain":["Videos:   0%|          | 0/1518 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc439cd1d2a246338ac6ec412ade7e85"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["All videos processed successfully.\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c2zQ9MyriTHA","executionInfo":{"status":"ok","timestamp":1759453731641,"user_tz":240,"elapsed":14062,"user":{"displayName":"Patekj","userId":"15093661387757056260"}},"outputId":"c297a757-febd-4d72-ec79-7728cb113ac3"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]}]}